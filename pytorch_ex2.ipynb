{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "identical-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "digital-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "cpu_tensor = torch.zeros(2,3)\n",
    "device = torch.device(\"cuda:0\")\n",
    "gpu_tensor = cpu_tensor.to(device)\n",
    "print(gpu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pressed-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0], requires_grad = True)\n",
    "b = torch.tensor([3.0], requires_grad = True)\n",
    "c = torch.tensor([5.0], requires_grad = True)\n",
    "d = torch.tensor([10.0], requires_grad = True)\n",
    "u = a*b\n",
    "t = torch.log(d)\n",
    "v = t*c\n",
    "t.retain_grad() # to get/remember grad for non-leaf tensor\n",
    "e = u*v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wrong-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_leaf: True\n",
      "a.grad_fn: None\n",
      "a.grad: None\n",
      "\n",
      "b.is_leaf: True\n",
      "b.grad_fn: None\n",
      "b.grad: None\n",
      "\n",
      "c.is_leaf: True\n",
      "c.grad_fn: None\n",
      "c.grad: None\n",
      "\n",
      "d.is_leaf: True\n",
      "d.grad_fn: None\n",
      "d.grad: None\n",
      "\n",
      "e.is_leaf: False\n",
      "e.grad_fn: <MulBackward0 object at 0x000001CC9DEE7400>\n",
      "e.grad: None\n",
      "\n",
      "u.is_leaf: False\n",
      "u.grad_fn: <MulBackward0 object at 0x000001CC9DEE7518>\n",
      "u.grad: None\n",
      "\n",
      "v.is_leaf: False\n",
      "v.grad_fn: <MulBackward0 object at 0x000001CC9DEE7518>\n",
      "v.grad: None\n",
      "\n",
      "t.is_leaf: False\n",
      "t.grad_fn: <LogBackward object at 0x000001CC9DDFEBE0>\n",
      "t.grad: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
     ]
    }
   ],
   "source": [
    "print(f'a.is_leaf: {a.is_leaf}')\n",
    "print(f'a.grad_fn: {a.grad_fn}')\n",
    "print(f'a.grad: {a.grad}')\n",
    "print()\n",
    "\n",
    "print(f'b.is_leaf: {b.is_leaf}')\n",
    "print(f'b.grad_fn: {b.grad_fn}')\n",
    "print(f'b.grad: {b.grad}')\n",
    "print()\n",
    "\n",
    "print(f'c.is_leaf: {c.is_leaf}')\n",
    "print(f'c.grad_fn: {c.grad_fn}')\n",
    "print(f'c.grad: {c.grad}')\n",
    "print()\n",
    "\n",
    "print(f'd.is_leaf: {d.is_leaf}')\n",
    "print(f'd.grad_fn: {d.grad_fn}')\n",
    "print(f'd.grad: {d.grad}')\n",
    "print()\n",
    "\n",
    "print(f'e.is_leaf: {e.is_leaf}')\n",
    "print(f'e.grad_fn: {e.grad_fn}')\n",
    "print(f'e.grad: {e.grad}')\n",
    "print()\n",
    "\n",
    "print(f'u.is_leaf: {u.is_leaf}')\n",
    "print(f'u.grad_fn: {u.grad_fn}')\n",
    "print(f'u.grad: {u.grad}')\n",
    "print()\n",
    "\n",
    "print(f'v.is_leaf: {v.is_leaf}')\n",
    "print(f'v.grad_fn: {v.grad_fn}')\n",
    "print(f'v.grad: {v.grad}')\n",
    "print()\n",
    "\n",
    "print(f't.is_leaf: {t.is_leaf}')\n",
    "print(f't.grad_fn: {t.grad_fn}')\n",
    "print(f't.grad: {t.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "covered-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "opponent-canada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial a} = 34.538780212402344$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial b} = 23.02585220336914$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial c} = 13.815510749816895$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial d} = 3.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial a}} = {a.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial b}} = {b.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial c}} = {c.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial d}} = {d.grad.item()}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "indirect-founder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "former-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_leaf: True\n",
      "a.grad_fn: None\n",
      "a.grad: tensor([34.5388])\n",
      "\n",
      "b.is_leaf: True\n",
      "b.grad_fn: None\n",
      "b.grad: tensor([23.0259])\n",
      "\n",
      "c.is_leaf: True\n",
      "c.grad_fn: None\n",
      "c.grad: tensor([13.8155])\n",
      "\n",
      "d.is_leaf: True\n",
      "d.grad_fn: None\n",
      "d.grad: tensor([3.])\n",
      "\n",
      "e.is_leaf: False\n",
      "e.grad_fn: <MulBackward0 object at 0x000001CCD0DE7EF0>\n",
      "e.grad: None\n",
      "\n",
      "u.is_leaf: False\n",
      "u.grad_fn: <MulBackward0 object at 0x000001CCD0DE7EF0>\n",
      "u.grad: None\n",
      "\n",
      "v.is_leaf: False\n",
      "v.grad_fn: <MulBackward0 object at 0x000001CCD0DE7EF0>\n",
      "v.grad: None\n",
      "\n",
      "t.is_leaf: False\n",
      "t.grad_fn: <LogBackward object at 0x000001CCD0DE7EF0>\n",
      "t.grad: tensor([30.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
     ]
    }
   ],
   "source": [
    "print(f'a.is_leaf: {a.is_leaf}')\n",
    "print(f'a.grad_fn: {a.grad_fn}')\n",
    "print(f'a.grad: {a.grad}')\n",
    "print()\n",
    "\n",
    "print(f'b.is_leaf: {b.is_leaf}')\n",
    "print(f'b.grad_fn: {b.grad_fn}')\n",
    "print(f'b.grad: {b.grad}')\n",
    "print()\n",
    "\n",
    "print(f'c.is_leaf: {c.is_leaf}')\n",
    "print(f'c.grad_fn: {c.grad_fn}')\n",
    "print(f'c.grad: {c.grad}')\n",
    "print()\n",
    "\n",
    "print(f'd.is_leaf: {d.is_leaf}')\n",
    "print(f'd.grad_fn: {d.grad_fn}')\n",
    "print(f'd.grad: {d.grad}')\n",
    "print()\n",
    "\n",
    "print(f'e.is_leaf: {e.is_leaf}')\n",
    "print(f'e.grad_fn: {e.grad_fn}')\n",
    "print(f'e.grad: {e.grad}')\n",
    "print()\n",
    "\n",
    "print(f'u.is_leaf: {u.is_leaf}')\n",
    "print(f'u.grad_fn: {u.grad_fn}')\n",
    "print(f'u.grad: {u.grad}')\n",
    "print()\n",
    "\n",
    "print(f'v.is_leaf: {v.is_leaf}')\n",
    "print(f'v.grad_fn: {v.grad_fn}')\n",
    "print(f'v.grad: {v.grad}')\n",
    "print()\n",
    "\n",
    "print(f't.is_leaf: {t.is_leaf}')\n",
    "print(f't.grad_fn: {t.grad_fn}')\n",
    "print(f't.grad: {t.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cross-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\haenm\\anaconda3\\envs\\bt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "exclusive-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant(2.0)\n",
    "b=tf.constant(3.0)\n",
    "c=tf.constant(5.0)\n",
    "d=tf.constant(10.0)\n",
    "\n",
    "u = tf.multiply(a,b,name='u')\n",
    "t = tf.log(d)\n",
    "v = tf.multiply(t,c,name='v')\n",
    "e = tf.add(u,v,name='e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adaptive-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"u:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Log:0\", shape=(), dtype=float32)\n",
      "Tensor(\"v:0\", shape=(), dtype=float32)\n",
      "Tensor(\"e:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(u,t,v,e,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "legitimate-conversion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial a} = [3.0]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial b} = [2.0]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial c} = [2.3025851]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial d} = [0.5]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    display(Math(fr'\\frac{{\\partial e}}{{\\partial a}} = {sess.run(tf.gradients(e,a))}'))\n",
    "    print()\n",
    "    display(Math(fr'\\frac{{\\partial e}}{{\\partial b}} = {sess.run(tf.gradients(e,b))}'))\n",
    "    print()\n",
    "    display(Math(fr'\\frac{{\\partial e}}{{\\partial c}} = {sess.run(tf.gradients(e,c))}'))\n",
    "    print()\n",
    "    display(Math(fr'\\frac{{\\partial e}}{{\\partial d}} = {sess.run(tf.gradients(e,d))}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-constant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
